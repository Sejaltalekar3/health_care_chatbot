{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yqjRPZQ64um",
        "outputId": "282acbe6-a49d-450f-a803-22d66241e096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'medical-chatbot'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 17 (delta 0), reused 0 (delta 0), pack-reused 16 (from 1)\u001b[K\n",
            "Receiving objects: 100% (17/17), 21.25 MiB | 16.22 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AIwithhassan/medical-chatbot.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/medical-chatbot/connect_memory_with_llm.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmJjwOeK67fx",
        "outputId": "15d66d0e-146a-45fb-dccb-37d2c23b5425"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-09 05:17:47.591488: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744175867.643188    5260 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744175867.659072    5260 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-09 05:17:47.711180: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/medical-chatbot/connect_memory_with_llm.py\", line 47, in <module>\n",
            "    db=FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_community/vectorstores/faiss.py\", line 1205, in load_local\n",
            "    index = faiss.read_index(str(path / f\"{index_name}.faiss\"))\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/faiss/swigfaiss_avx2.py\", line 10947, in read_index\n",
            "    return _swigfaiss_avx2.read_index(*args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Error in faiss::FileIOReader::FileIOReader(const char*) at /project/faiss/faiss/impl/io.cpp:67: Error: 'f' failed: could not open vectorstore/db_faiss/index.faiss for reading: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUNt_8Z67GcP",
        "outputId": "71992216-34ea-407f-903f-635e604d2df8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Set your PDF data path and FAISS output path\n",
        "DATA_PATH = \"/content/medical-chatbot/data\"\n",
        "DB_FAISS_PATH = \"vectorstore/db_faiss\"\n",
        "\n",
        "# Step 1: Load PDF files safely\n",
        "def load_pdf_files(data_path):\n",
        "    try:\n",
        "        loader = DirectoryLoader(data_path, glob='*.pdf', loader_cls=PyPDFLoader)\n",
        "        documents = loader.load()\n",
        "        print(f\"‚úÖ Loaded {len(documents)} pages from PDF files.\")\n",
        "        return documents\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading PDFs: {e}\")\n",
        "        return []\n",
        "\n",
        "documents = load_pdf_files(DATA_PATH)\n",
        "\n",
        "# Step 2: Split documents into manageable text chunks\n",
        "def create_chunks(extracted_docs):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    chunks = text_splitter.split_documents(extracted_docs)\n",
        "    print(f\"‚úÖ Created {len(chunks)} text chunks.\")\n",
        "    return chunks\n",
        "\n",
        "text_chunks = create_chunks(documents)\n",
        "\n",
        "# Step 3: Load the embedding model\n",
        "def get_embedding_model():\n",
        "    return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "embedding_model = get_embedding_model()\n",
        "\n",
        "# Step 4: Create FAISS index and save it\n",
        "def save_faiss_index(chunks, embed_model, save_path):\n",
        "    try:\n",
        "        if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "        db = FAISS.from_documents(chunks, embed_model)\n",
        "        db.save_local(save_path)\n",
        "        print(f\"‚úÖ FAISS index saved to: {save_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating/saving FAISS index: {e}\")\n",
        "\n",
        "save_faiss_index(text_chunks, embedding_model, DB_FAISS_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCsu9THc7J-C",
        "outputId": "e4e085a9-3f4c-4820-cbc0-35657b704610"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 10759 pages from PDF files.\n",
            "‚úÖ Created 80909 text chunks.\n",
            "‚úÖ FAISS index saved to: vectorstore/db_faiss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGRDt6nj_WEc",
        "outputId": "9918b04d-69c4-43b5-da63-0710fdbd55cd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
        "\n",
        "# Step 1: Load HF Token & Model ID\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"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\")  # Replace with os.getenv in production\n",
        "HUGGINGFACE_REPO_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "DB_FAISS_PATH = \"vectorstore/db_faiss\"\n",
        "\n",
        "# Step 2: Load LLM with proper token usage\n",
        "def load_llm():\n",
        "    return HuggingFaceEndpoint(\n",
        "        repo_id=HUGGINGFACE_REPO_ID,\n",
        "        task=\"text-generation\",\n",
        "        huggingfacehub_api_token=HF_TOKEN,\n",
        "        temperature=0.5,\n",
        "        max_new_tokens=512  # ‚úÖ FIXED: Moved out of model_kwargs\n",
        "    )\n",
        "\n",
        "# Step 3: Prompt Template\n",
        "CUSTOM_PROMPT_TEMPLATE = \"\"\"\n",
        "Use the pieces of information provided in the context to answer user's question.\n",
        "If you don‚Äôt know the answer, just say that you don‚Äôt know ‚Äì don‚Äôt try to make up an answer.\n",
        "Don‚Äôt provide anything out of the given context.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Start the answer directly. No small talk please.\n",
        "\"\"\"\n",
        "\n",
        "def get_custom_prompt():\n",
        "    return PromptTemplate(template=CUSTOM_PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# Step 4: Load Vector DB\n",
        "def load_vector_db():\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    db = FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)\n",
        "    return db\n",
        "\n",
        "# Step 5: Setup RetrievalQA Chain\n",
        "def build_qa_chain():\n",
        "    retriever = load_vector_db().as_retriever(search_kwargs={\"k\": 3})\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=load_llm(),\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": get_custom_prompt()}\n",
        "    )\n",
        "    return qa_chain\n",
        "\n",
        "# Step 6: Query interface\n",
        "if __name__ == \"__main__\":\n",
        "    user_query = input(\"üìù Write Query Here: \")\n",
        "    qa_chain = build_qa_chain()\n",
        "    result = qa_chain.invoke({\"query\": user_query})\n",
        "\n",
        "    print(\"\\n‚úÖ RESULT:\\n\", result[\"result\"])\n",
        "    print(\"\\nüìÑ SOURCE DOCUMENTS:\\n\")\n",
        "    for doc in result[\"source_documents\"]:\n",
        "        print(f\"- {doc.metadata.get('source', 'N/A')}: {doc.page_content[:200]}...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A01WaZP6_fgi",
        "outputId": "d9702d92-3bd0-4b10-9ac8-0c6558861721"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Write Query Here: i have vomiting sensation what food i should avoid\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ RESULT:\n",
            " \n",
            "You should avoid eating milk products, spicy food, and alcohol. Eating bland foods like crackers, soft drinks, BRAT diet (bananas, rice, apple-sauce, dry toast) can help.\n",
            "\n",
            "üìÑ SOURCE DOCUMENTS:\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: may prolong the infection.\n",
            "People with food poisoning should modify their diet.\n",
            "During period of active vomiting and diarrhea they\n",
            "should not try to eat and should drink only clear liquids\n",
            "frequently ...\n",
            "\n",
            "- /content/medical-chatbot/data/The_Merck_Manual_of_Diagnosis_and_Therapy_2011 - 19th Edn........pdf: only bland foods (eg, crackers, soft drinks, BRAT diet [bananas, rice, apple-sauce, dry toast]) should be\n",
            "eaten. Eating before rising may help. If dehydration (eg, due to hyperemesis gravidarum) is su...\n",
            "\n",
            "- /content/medical-chatbot/data/c46528ba033a8197e32c40887c398198.pdf: reported with intestinal obstruction. Gastroparesis can produce nausea within minutes of \n",
            "food consumption but, in severe cases, leads to vomiting of meal residue ingested \n",
            "hours or days previously. B...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
        "\n",
        "# Load HF token and model\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"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\")\n",
        "HUGGINGFACE_REPO_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "DB_FAISS_PATH = \"vectorstore/db_faiss\"\n",
        "\n",
        "# Load LLM\n",
        "def load_llm():\n",
        "    return HuggingFaceEndpoint(\n",
        "        repo_id=HUGGINGFACE_REPO_ID,\n",
        "        task=\"text-generation\",\n",
        "        huggingfacehub_api_token=HF_TOKEN,\n",
        "        temperature=0.5,\n",
        "        max_new_tokens=512\n",
        "    )\n",
        "\n",
        "# Prompt template\n",
        "CUSTOM_PROMPT_TEMPLATE = \"\"\"\n",
        "Use the pieces of information provided in the context to answer user's question.\n",
        "If you don‚Äôt know the answer, just say that you don‚Äôt know ‚Äì don‚Äôt try to make up an answer.\n",
        "Don‚Äôt provide anything out of the given context.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Start the answer directly. No small talk please.\n",
        "\"\"\"\n",
        "\n",
        "def get_custom_prompt():\n",
        "    return PromptTemplate(template=CUSTOM_PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# Load vector DB\n",
        "def load_vector_db():\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    db = FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)\n",
        "    return db\n",
        "\n",
        "# Setup RetrievalQA chain\n",
        "def build_qa_chain():\n",
        "    retriever = load_vector_db().as_retriever(search_kwargs={\"k\": 3})\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=load_llm(),\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": get_custom_prompt()}\n",
        "    )\n",
        "    return qa_chain\n",
        "\n",
        "# Check if input is a greeting or casual talk\n",
        "def is_small_talk(query):\n",
        "    greetings = [\"hi\", \"hello\", \"hey\", \"how are you\", \"good morning\", \"good evening\"]\n",
        "    return any(greet in query.lower() for greet in greetings)\n",
        "\n",
        "# Main conversation flow\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ü§ñ Hello! I‚Äôm your Virtual Doctor. Let‚Äôs get started.\")\n",
        "\n",
        "    name = input(\"üë§ May I know your name? \")\n",
        "    age = input(f\"üéÇ Hi {name}, may I know your age? \")\n",
        "\n",
        "    print(f\"\\nüßë‚Äç‚öïÔ∏è Thank you, {name} ({age} years old). You can now ask me anything related to health or diseases.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"\\nüìù Your medical question (or type 'exit' to quit): \").strip()\n",
        "\n",
        "        if user_query.lower() == \"exit\":\n",
        "            print(\"üëã Take care! Feel free to come back if you have more questions.\")\n",
        "            break\n",
        "\n",
        "        if is_small_talk(user_query):\n",
        "            print(\"ü§ó I'm here to help you with medical questions. Could you tell me what health topic you want to know about?\")\n",
        "            continue\n",
        "\n",
        "        qa_chain = build_qa_chain()\n",
        "        result = qa_chain.invoke({\"query\": user_query})\n",
        "\n",
        "        print(\"\\n‚úÖ RESULT:\\n\", result[\"result\"])\n",
        "        print(\"\\nüìÑ SOURCE DOCUMENTS:\\n\")\n",
        "        for doc in result[\"source_documents\"]:\n",
        "            print(f\"- {doc.metadata.get('source', 'N/A')}: {doc.page_content[:200]}...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRmnebCoBeVB",
        "outputId": "9e0cee0f-ade5-4972-c4c8-28a03c19107d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Hello! I‚Äôm your Virtual Doctor. Let‚Äôs get started.\n",
            "üë§ May I know your name? sejal\n",
            "üéÇ Hi sejal, may I know your age? 23\n",
            "\n",
            "üßë‚Äç‚öïÔ∏è Thank you, sejal (23 years old). You can now ask me anything related to health or diseases.\n",
            "\n",
            "üìù Your medical question (or type 'exit' to quit): i am facing fever what should i do\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ RESULT:\n",
            " \n",
            "You should seek medical attention immediately if you are experiencing a fever, especially if it is accompanied by other symptoms that could indicate a serious infection such as meningitis. If you are not experiencing any other symptoms and the fever is not very high, you can try to treat it at home by taking over-the-counter medications such as aspirin, acetaminophen (Tylenol), or ibuprofen (Advil) to reduce the fever. However, it is important to consult with a physician before taking any medication.\n",
            "\n",
            "üìÑ SOURCE DOCUMENTS:\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: A very high fever in a small child can trigger\n",
            "seizures (febrile seizures) and therefore should be treated\n",
            "immediately. A fever accompanied by the above symp-\n",
            "toms can indicate the presence of a serio...\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: cause of a fever.\n",
            "Treatment\n",
            "Physicians agree that the most effective treatment\n",
            "for a fever is to address its underlying cause, such as\n",
            "through the administration of antibiotics. Also, because\n",
            "a fever ...\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: Few symptoms in medicine present such a diagnos-\n",
            "tic challenge as fever. Nonetheless, if a careful, logical,\n",
            "and thorough evaluation is performed, a diagnosis will be\n",
            "found in most cases. The patient‚Äô...\n",
            "\n",
            "\n",
            "üìù Your medical question (or type 'exit' to quit): can you suggest me any medicines for fever\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ RESULT:\n",
            " \n",
            "Yes, you can take aspirin, acetaminophen (Tylenol), or ibuprofen (Advil) to lower fever. However, aspirin should not be given to a child or adolescent with a fever since it has been linked to an increased risk of Reye's syndrome.\n",
            "\n",
            "üìÑ SOURCE DOCUMENTS:\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: cause of a fever.\n",
            "Treatment\n",
            "Physicians agree that the most effective treatment\n",
            "for a fever is to address its underlying cause, such as\n",
            "through the administration of antibiotics. Also, because\n",
            "a fever ...\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: however, should not be given to a child or adolescent\n",
            "with a fever since this drug has been linked to an\n",
            "increased risk of Reye‚Äôs syndrome. Bathing a patient in\n",
            "cool water can also help alleviate a hi...\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: A very high fever in a small child can trigger\n",
            "seizures (febrile seizures) and therefore should be treated\n",
            "immediately. A fever accompanied by the above symp-\n",
            "toms can indicate the presence of a serio...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
        "\n",
        "# Load Hugging Face token and model\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"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\")\n",
        "HUGGINGFACE_REPO_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "DB_FAISS_PATH = \"vectorstore/db_faiss\"\n",
        "\n",
        "# Load the LLM\n",
        "def load_llm():\n",
        "    return HuggingFaceEndpoint(\n",
        "        repo_id=HUGGINGFACE_REPO_ID,\n",
        "        task=\"text-generation\",\n",
        "        huggingfacehub_api_token=HF_TOKEN,\n",
        "        temperature=0.5,\n",
        "        max_new_tokens=512\n",
        "    )\n",
        "\n",
        "# Custom Prompt Template\n",
        "CUSTOM_PROMPT_TEMPLATE = \"\"\"\n",
        "Use the pieces of information provided in the context to answer user's question.\n",
        "If you don‚Äôt know the answer, just say that you don‚Äôt know ‚Äì don‚Äôt try to make up an answer.\n",
        "Don‚Äôt provide anything out of the given context.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Start the answer directly. No small talk please.\n",
        "\"\"\"\n",
        "\n",
        "def get_custom_prompt():\n",
        "    return PromptTemplate(template=CUSTOM_PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# Load FAISS vector database\n",
        "def load_vector_db():\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    db = FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)\n",
        "    return db\n",
        "\n",
        "# Build RetrievalQA chain\n",
        "def build_qa_chain():\n",
        "    retriever = load_vector_db().as_retriever(search_kwargs={\"k\": 3})\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=load_llm(),\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": get_custom_prompt()}\n",
        "    )\n",
        "    return qa_chain\n",
        "\n",
        "# Smart classification: is the query smalltalk or medical?\n",
        "def classify_input(query):\n",
        "    query = query.lower()\n",
        "    smalltalk_keywords = [\n",
        "        \"thank\", \"thanks\", \"hello\", \"hi\", \"how are you\", \"good morning\",\n",
        "        \"good evening\", \"bye\", \"ok\", \"nice\", \"cool\", \"goodbye\", \"see you\"\n",
        "    ]\n",
        "    smalltalk_count = sum(1 for word in smalltalk_keywords if word in query)\n",
        "\n",
        "    # If short and mostly greeting, it's smalltalk\n",
        "    if len(query.split()) <= 4 and smalltalk_count > 0:\n",
        "        return \"smalltalk\"\n",
        "\n",
        "    # Otherwise treat as medical\n",
        "    return \"medical\"\n",
        "\n",
        "# Main chatbot loop\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ü§ñ Hello! I‚Äôm your Virtual Doctor. Let‚Äôs get started.\")\n",
        "\n",
        "    name = input(\"üë§ May I know your name? \")\n",
        "    age = input(f\"üéÇ Hi {name}, may I know your age? \")\n",
        "\n",
        "    print(f\"\\nüßë‚Äç‚öïÔ∏è Thank you, {name} ({age} years old). You can now ask me anything related to health or diseases.\")\n",
        "\n",
        "    qa_chain = build_qa_chain()\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"\\nüìù Your question (or type 'exit' to quit): \").strip()\n",
        "\n",
        "        if user_query.lower() == \"exit\":\n",
        "            print(\"üëã Take care! Feel free to come back if you have more questions.\")\n",
        "            break\n",
        "\n",
        "        query_type = classify_input(user_query)\n",
        "\n",
        "        if query_type == \"smalltalk\":\n",
        "            print(\"ü§ó Hello again! What would you like to know today?\")\n",
        "            continue\n",
        "\n",
        "        result = qa_chain.invoke({\"query\": user_query})\n",
        "\n",
        "        print(\"\\n‚úÖ RESULT:\\n\", result[\"result\"])\n",
        "        print(\"\\nüìÑ SOURCE DOCUMENTS:\\n\")\n",
        "        for doc in result[\"source_documents\"]:\n",
        "            print(f\"- {doc.metadata.get('source', 'N/A')}: {doc.page_content[:200]}...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sr80huZwIKqi",
        "outputId": "735edcfa-013b-4813-db96-925b1549b443"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Hello! I‚Äôm your Virtual Doctor. Let‚Äôs get started.\n",
            "üë§ May I know your name? sejal\n",
            "üéÇ Hi sejal, may I know your age? 23\n",
            "\n",
            "üßë‚Äç‚öïÔ∏è Thank you, sejal (23 years old). You can now ask me anything related to health or diseases.\n",
            "\n",
            "üìù Your question (or type 'exit' to quit): hello dr i have fever\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ RESULT:\n",
            " \n",
            "Hello! I'm here to help. It sounds like you have a fever. Based on the information provided, it's important to first determine if the fever is due to an infection. Symptoms such as sudden onset and fluctuation throughout the day, or a fever that lasts for several weeks, could indicate an autoimmune disease or cancer. However, a very high fever in a small child can trigger seizures and should be treated immediately. If your fever is accompanied by other symptoms such as severe headache, stiff neck, confusion, or difficulty speaking, these could indicate a serious infection like meningitis and should be brought to the immediate attention of a physician. If your fever persists, it would be a good idea to consult with a healthcare professional for a proper diagnosis and treatment.\n",
            "\n",
            "üìÑ SOURCE DOCUMENTS:\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: Few symptoms in medicine present such a diagnos-\n",
            "tic challenge as fever. Nonetheless, if a careful, logical,\n",
            "and thorough evaluation is performed, a diagnosis will be\n",
            "found in most cases. The patient‚Äô...\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: the patient, and his or her overall health. Most fevers\n",
            "caused by infections are acute, appearing suddenly and\n",
            "then dissipating as the immune system defeats the infec-\n",
            "tious agent. An infectious fever...\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: A very high fever in a small child can trigger\n",
            "seizures (febrile seizures) and therefore should be treated\n",
            "immediately. A fever accompanied by the above symp-\n",
            "toms can indicate the presence of a serio...\n",
            "\n",
            "\n",
            "üìù Your question (or type 'exit' to quit): i feel like vomiting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ RESULT:\n",
            " \n",
            "The context does not provide information about vomiting without any other symptoms. However, it does mention that nausea and vomiting may accompany sentinel headaches or when an aneurysm ruptures. If you are experiencing a sudden, extremely severe headache, numbness and tingling around the lips, tongue, and mouth, itching, dry mouth, a metallic taste in the mouth, blurry vision, temporary blindness, a slow pulse, feeling that your teeth are loose, and reversal of hot and cold sensations on the skin, you should seek immediate medical attention, as these could be symptoms of a ruptured aneurysm. If your symptoms are not accompanied by these, it's best to consult a healthcare professional to determine the cause of your nausea and vomiting.\n",
            "\n",
            "üìÑ SOURCE DOCUMENTS:\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: called prokinate agents. A review of eating habits (e.g.\n",
            "chewing with the mouth open, gulping food, or talking\n",
            "while chewing) may reveal a tendency to swallow air. This\n",
            "may contribute to feeling bloat...\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: referred to as sentinel headaches. Nausea, vomiting, and\n",
            "dizziness may accompany sentinel headaches. Unfortu-\n",
            "nately, these symptoms can be confused with tension\n",
            "headaches or migraines, and treatment ...\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: watery diarrhea . The most characteristic symptoms of\n",
            "the illness are those involving the nervous system. These\n",
            "include numbness and tingling around the lips, tongue,\n",
            "and mouth; itching ; dry mouth ; ...\n",
            "\n",
            "\n",
            "üìù Your question (or type 'exit' to quit): i have a cut in my hand\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ RESULT:\n",
            " \n",
            "For a cut in your hand, it is important to clean the wound area with a saline (salt water) solution. After cleaning, you may apply a topical anesthetic gel to minimize pain. However, if the wound is deep or large, it is recommended to seek medical attention from a physician assistant or a physician. They may perform a procedure to remove any dead tissue and stitch the skin edges to close the wound.\n",
            "\n",
            "üìÑ SOURCE DOCUMENTS:\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: physician assistant may perform the procedure.\n",
            "The physician will begin by flushing the area with a\n",
            "saline (salt water) solution, and then will apply a topical\n",
            "anesthetic gel to the edges of the wound...\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: movement may be partially restricted due to swelling of\n",
            "the affected area. Tendon injuries usually result in the\n",
            "loss of ability to straighten or bend the finger.\n",
            "Treatment\n",
            "Amputation with bone and un...\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: pulled back, and then it is cut away from the tip of the\n",
            "penis. Stitches are usually used to close the skin edges.\n",
            "Preparation\n",
            "Despite a long-standing belief that infants do not\n",
            "experience serious pai...\n",
            "\n",
            "\n",
            "üìù Your question (or type 'exit' to quit): thanks\n",
            "ü§ó Hello again! What would you like to know today?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-116f2eef6e13>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0muser_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüìù Your question (or type 'exit' to quit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
        "\n",
        "# Load HF token and model\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"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\")\n",
        "HUGGINGFACE_REPO_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "DB_FAISS_PATH = \"vectorstore/db_faiss\"\n",
        "\n",
        "# Load LLM\n",
        "def load_llm():\n",
        "    return HuggingFaceEndpoint(\n",
        "        repo_id=HUGGINGFACE_REPO_ID,\n",
        "        task=\"text-generation\",\n",
        "        huggingfacehub_api_token=HF_TOKEN,\n",
        "        temperature=0.5,\n",
        "        max_new_tokens=512\n",
        "    )\n",
        "\n",
        "# Prompt template\n",
        "CUSTOM_PROMPT_TEMPLATE = \"\"\"\n",
        "Use the pieces of information provided in the context to answer user's question.\n",
        "If you don‚Äôt know the answer, just say that you don‚Äôt know ‚Äì don‚Äôt try to make up an answer.\n",
        "Don‚Äôt provide anything out of the given context.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Start the answer directly. No small talk please.\n",
        "\"\"\"\n",
        "\n",
        "def get_custom_prompt():\n",
        "    return PromptTemplate(template=CUSTOM_PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# Load vector DB\n",
        "def load_vector_db():\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    db = FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)\n",
        "    return db\n",
        "\n",
        "# Setup RetrievalQA chain\n",
        "def build_qa_chain():\n",
        "    retriever = load_vector_db().as_retriever(search_kwargs={\"k\": 3})\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=load_llm(),\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": get_custom_prompt()}\n",
        "    )\n",
        "    return qa_chain\n",
        "\n",
        "# Classify input\n",
        "def classify_input(query):\n",
        "    query = query.lower()\n",
        "\n",
        "    gratitude_keywords = [\"thank\", \"thanks\", \"thank you\", \"appreciate\", \"grateful\"]\n",
        "    greeting_keywords = [\"hello\", \"hi\", \"how are you\", \"good morning\", \"good evening\", \"hey\"]\n",
        "    farewell_keywords = [\"bye\", \"goodbye\", \"see you\", \"later\"]\n",
        "\n",
        "    if any(word in query for word in gratitude_keywords):\n",
        "        return \"gratitude\"\n",
        "    elif any(word in query for word in farewell_keywords):\n",
        "        return \"farewell\"\n",
        "    elif len(query.split()) <= 4 and any(word in query for word in greeting_keywords):\n",
        "        return \"greeting\"\n",
        "    else:\n",
        "        return \"medical\"\n",
        "\n",
        "# Main conversation flow\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ü§ñ Hello! I‚Äôm your Virtual Doctor. Let‚Äôs get started.\")\n",
        "\n",
        "    name = input(\"üë§ May I know your name? \")\n",
        "    age = input(f\"üéÇ Hi {name}, may I know your age? \")\n",
        "\n",
        "    print(f\"\\nüßë‚Äç‚öïÔ∏è Thank you, {name} ({age} years old). You can now ask me anything related to health or diseases.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"\\nüìù Your question (or type 'exit' to quit): \").strip()\n",
        "\n",
        "        if user_query.lower() == \"exit\":\n",
        "            print(\"üëã Take care! Feel free to come back if you have more questions.\")\n",
        "            break\n",
        "\n",
        "        query_type = classify_input(user_query)\n",
        "\n",
        "        if query_type == \"greeting\":\n",
        "            print(\"ü§ó Hello again! What would you like to know today?\")\n",
        "            continue\n",
        "        elif query_type == \"gratitude\":\n",
        "            print(\"üôè You're welcome! If you have any more questions, feel free to ask.\")\n",
        "            continue\n",
        "        elif query_type == \"farewell\":\n",
        "            print(\"üëã Goodbye! Stay healthy and take care.\")\n",
        "            break\n",
        "\n",
        "        qa_chain = build_qa_chain()\n",
        "        result = qa_chain.invoke({\"query\": user_query})\n",
        "\n",
        "        print(\"\\n‚úÖ RESULT:\\n\", result[\"result\"])\n",
        "        print(\"\\nüìÑ SOURCE DOCUMENTS:\\n\")\n",
        "        for doc in result[\"source_documents\"]:\n",
        "            print(f\"- {doc.metadata.get('source', 'N/A')}: {doc.page_content[:200]}...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k6Fd_AVJJGpi",
        "outputId": "2ec1ea11-b44a-494c-fc33-ec0a192da66c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Hello! I‚Äôm your Virtual Doctor. Let‚Äôs get started.\n",
            "üë§ May I know your name? sejal\n",
            "üéÇ Hi sejal, may I know your age? 23\n",
            "\n",
            "üßë‚Äç‚öïÔ∏è Thank you, sejal (23 years old). You can now ask me anything related to health or diseases.\n",
            "\n",
            "üìù Your question (or type 'exit' to quit): i am facing vomiting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ RESULT:\n",
            " \n",
            "Your symptoms of vomiting may be due to a digestive problem. It's possible that the cause could be bacterial or viral infection, peptic ulcer, gallbladder, or liver disease. To determine the exact cause, it's recommended to consult a healthcare professional for an investigation.\n",
            "\n",
            "üìÑ SOURCE DOCUMENTS:\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: turbed digestion, which may be accompanied by symp-\n",
            "toms such as nausea and vomiting , heartburn , bloat-\n",
            "ing, and stomach discomfort.\n",
            "Causes and symptoms\n",
            "The digestive problems may have an identifiab...\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: the region of the gallbladder and liver. The pain may be\n",
            "crampy and episodic, or it may be constant. The pain is\n",
            "often described as pushing through to the right upper\n",
            "back and shoulder. Because deep b...\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: and/or diarrhea. Some food-borne toxins can affect the\n",
            "nervous system.\n",
            "Description\n",
            "Every year millions of people suffer from bouts of\n",
            "vomiting and diarrhea each year that they blame on\n",
            "‚Äúsomething I at...\n",
            "\n",
            "\n",
            "üìù Your question (or type 'exit' to quit): tablets for vomiting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ RESULT:\n",
            " \n",
            "One anti-nausea medication that may be used is Kytril, which comes in tablet form. However, it's important to note that the context does not mention any tablets specifically for inducing vomiting.\n",
            "\n",
            "üìÑ SOURCE DOCUMENTS:\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: Some patients bring a friend or family member to provide\n",
            "company and support during treatment.\n",
            "Sometimes, patients taking chemotherapy drugs\n",
            "known to cause nausea are given medications called anti-\n",
            "em...\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: ‚Ä¢ Elimination of the drug that has not yet been absorbed is\n",
            "attempted. V omiting may be induced using ipecac syrup\n",
            "or other drugs that cause vomiting. Ipecac syrup should\n",
            "not be given to patients who ...\n",
            "\n",
            "- /content/medical-chatbot/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf: tains one of the active ingredients of marijuana is avail-\n",
            "able with a physician‚Äôs prescription and sometimes is\n",
            "used to treat nausea and vomiting in patients undergoing\n",
            "cancer treatment. However, the...\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e429a32181a8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0muser_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüìù Your question (or type 'exit' to quit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the source and destination paths\n",
        "src_path = 'vectorstore/db_faiss'\n",
        "dst_path = '/content/drive/MyDrive/db_faiss'  # Change folder name if needed\n",
        "\n",
        "# Copy the folder\n",
        "shutil.copytree(src_path, dst_path)\n",
        "\n",
        "print(\"Folder copied to Google Drive.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GjvMNvPKGZK",
        "outputId": "3cb59ca9-bf8b-47f9-f735-ed1ce673d95f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Folder copied to Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aLBqQYzfiEN0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}